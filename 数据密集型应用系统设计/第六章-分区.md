# 第六章-分区

## 分区与复制

数据库分区，经常是分区和复制一起用，对各个分区复制好几份放在不同的节点上

上文中的**分区(partition)**，在MongoDB，Elasticsearch和Solr Cloud中被称为**分片(shard)**，在HBase中称之为**区域(Region)**，Bigtable中则是 \**表块（tablet）**，Cassandra和Riak中是**虚节点（vnode)，Couchbase中叫做**虚桶(vBucket)**。但是**分区(partitioning)** 是最约定俗成的叫法。

**一些术语**

- 偏斜：如果分区是不公平的，一些分区比其他分区有更多的数据或查询，我们称之为**偏斜（skew）**

- 热点：在极端的情况下，所有的负载可能压在一个分区上，其余9个节点空闲的，瓶颈落在这一个繁忙的节点上。不均衡导致的高负载的分区被称为**热点（hot spot）**。

  

## 键值数据的分区

1. ==根据键的范围分区==：这种方法容易导致热点，比如键是时间，就会导致某一段时间所在节点成为热点，可以加入别的列作为键来减轻热点负载
2. ==根据键的散列分区==：一致性哈希 == 散列分区；通过使用键散列进行分区，我们失去了键范围分区的一个很好的属性：高效执行范围查询的能力。曾经相邻的键现在分散在所有分区中，所以它们之间的顺序就丢失了。在MongoDB中，如果您使用了基于散列的分区模式，则任何范围查询都必须发送到所有分区；
   1. 折中做法：组合索引方法为一对多关系提供了一个优雅的数据模型。例如，在社交媒体网站上，一个用户可能会发布很多更新。如果更新的主键被选择为`(user_id, update_timestamp)`，那么您可以有效地检索特定用户在某个时间间隔内按时间戳排序的所有更新。不同的用户可以存储在不同的分区上，对于每个用户，更新按时间戳顺序存储在单个分区上。

3. ==负载偏斜与热点消除==：哈希分区可以帮助减少热点。但是，它不能完全避免它们：在极端情况下，所有的读写操作都是针对同一个键的，所有的请求都会被路由到同一个分区。如果一个主键被认为是非常火爆的，一个简单的方法是在==主键的开始或结尾添加一个随机数==。只要一个两位数的十进制随机数就可以**将主键分散**为100种不同的主键,==从而存储在不同的分区中==。



## 分区与次级索引

二级索引是关系数据库的必备特性，在文档数据库中应用也非常普遍。但考虑到其复杂性，许多键-值存储(如 HBase和 Voldemort)并不支持二级索引；但其他一些如Riak则开始增加对二级索引的支持。此外，二级索引技术也是Solr和 Elasticsearch等全文索引服务器存在之根本。
二级索引带来的主要挑战是它们不能规整的地映射到分区中。有两种主要的方法来支持对二级索引进行分区：基于文档的分区和基于词条的分区。

### 基于文档分区的二级索引

![img](https://gitee.com/Lighters_c/picgo-bed/raw/master/fig6-4.png)

每个分区完全独立，各自维护自己的二级索引，且只负责自己分
区内的文档而不关心其他分区中数据。文档分区素引也被称为本地
索引，而不是全局索引。

缺点是，直接对二级索引进行查找的时候，需要向所有分区发送请求。

这种查询分区数据库的方法有时被称为**分散/聚集（scatter/gather）**

它被广泛使用：MongoDB，Riak ，Cassandra，Elasticsearch，SolrCloud和VoltDB都使用文档分区二级索引。

### 基于词条分区的二级索引

![img](https://gitee.com/Lighters_c/picgo-bed/raw/master/fig6-5.png)

 关键词分区的全局索引优于文档分区索引的地方点是它可以使读取更有效率：不需要**分散/收集**所有分区，客户端只需要向包含关键词的分区发出请求。全局索引的缺点在于写入速度较慢且较为复杂，因为写入单个文档现在可能会影响索引的多个分区（文档中的每个关键词可能位于不同的分区或者不同的节点上） 。

全局关键词分区索引的其他用途包括Riak的搜索功能和Oracle数据仓库，它允许您在本地和全局索引之间进行选择。



## 分区再平衡

将负载从集群中的一个节点向另一个节点移动的过程称为**再平衡（rebalancing）**。

无论使用哪种分区方案，再平衡通常都要满足一些最低要求：

- 再平衡之后，负载（数据存储，读取和写入请求）应该在集群中的节点之间公平地共享。
- 再平衡发生时，数据库应该继续接受读取和写入。
- 节点之间只移动必须的数据，以便快速再平衡，并减少网络和磁盘I/O负载。

1. 反面教材：hash mod N

   - 如果N发生变化，这些索引就全失效了

2. 固定数量的分区

   ![img](https://gitee.com/Lighters_c/picgo-bed/raw/master/fig6-6.png)

   - 指定比节点数多的分区数，新增节点时直接把旧节点上的一些分区移动到新节点上，删除节点的时候就反过来

   -  在这种配置中，分区的数量通常在数据库第一次建立时确定，之后不会改变。当分区大小“恰到好处”的时候才能获得很好的性能。

3. 动态分区

   对于使用键范围分区的数据库（请参阅“[根据键的范围分区](http://ddia.vonng.com/#/ch6?id=根据键的范围分区)”），具有固定边界的固定数量的分区将非常不便

   - 按键的范围进行分区的数据库（如HBase和RethinkDB）会动态创建分区。当分区增长到超过配置的大小时（在HBase上，默认值是10GB），会被分成两个分区，如果大量数据被删除并且分区缩小到某个阈值以下，则可以将其与相邻分区合并。此过程与B树顶层发生的过程类似
   - 为了避免空数据库的时候全写在一个节点上，可以采用预分割，即在空数据库上配置一组初始分区。
   -  动态分区不仅==适用于数据的范围分区==，而且==也适用于散列分区==。从版本2.4开始，MongoDB同时支持范围和散列分区，并且都支持动态分割分区。

4. 按节点比例分区

   通过**动态分区**，分区的数量与数据集的大小成正比，因为拆分和合并过程将每个分区的大小保持在固定的最小值和最大值之间。另一方面，对于**固定数量**的分区，每个分区的大小与数据集的大小成正比。**在这两种情况下，分区的数量都与节点的数量无关。**

   Cassandra和Ketama使用的第三种方法是使==分区数与节点数成正比==——换句话说，**每个节点具有固定数量的分区**

   - 当一个新节点加入集群时，它随机选择固定数量的现有分区进行拆分，然后占有这些拆分分区中每个分区的一半，同时将每个分区的另一半留在原地。需要一种算法解决分割的时候给新节点分去了太多负载的问题。即公平分割
   - 随机选择分区边界要求使用基于散列的分区

## 请求路由

**服务发现(service discovery)** ，它不仅限于数据库。任何可通过网络访问的软件都有这个问题，特别是如果它的目标是高可用性（在多台机器上运行冗余配置）。

- 允许客户联系任何节点（例如，通过**循环策略的负载均衡（Round-Robin Load Balancer）**）。
- 首先将所有来自客户端的请求发送到路由层，由路由层负责将请求转发到对应的分区节点上（**我们的论文好像主要是这种方式**）路由层本身不处理任何请求，它仅充一个分区感知的负载均衡器。
- 要求客户端知道分区和节点的分配。在这种情况下，客户端可以直接连接到适当的节点，而不需要任何中介。

许多分布式数据系统都依赖于一个独立的协调服务，比如ZooKeeper来跟踪集群元数据，如[图6-8](http://ddia.vonng.com/#/img/fig6-8.png)所示。 每个节点在ZooKeeper中注册自己，ZooKeeper维护分区到节点的可靠映射。 其他参与者（如路由层或分区感知客户端）可以在ZooKeeper中订阅此信息。 只要分区分配发生了改变，或者集群中添加或删除了一个节点，ZooKeeper就会通知路由层使路由信息保持最新状态。

Cassandra和Riak则采用了不同的方法，它们在节点之间使用gossip协议来同步群集状态的变化。请求可以发送到任何节点，由该节点负责将其转发到目标分区节点。这种方式增加了数据库节点的复杂性，但是避免了对zooKeeper之类的外部协调服务的依赖。

Couchbase并不支持自动再平衡功能，这简化了设计。它通过配置一个名为moxi的路
由选择层，向集群节点学习最新的路由变化。