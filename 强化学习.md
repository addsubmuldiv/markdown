# 强化学习

## DQN

DQN是给定一个演员 $\pi$ 然后使用==神经网络==去拟合这个 $\pi$ 在某个状态 $s$ 的时候的价值，主要方法有两种，即 MC(蒙特卡洛) 和 TD(时序差分)

MC和TD的主要区别是：

- MC 每次都需要获得一个完整的 轨迹， 即演员 $\pi$ 从状态 $s$ 一直玩到游戏结束，然后网络得到的就是从这个状态一直玩到结束的奖励，对于输入状态 $s_a$ 就希望最后网络得到的值和 $G_a$ 越接近越好，对于输入状态 $s_b$ 就要网络得到的值越接近 $G_b$ 越好。
- TD 就是给一个状态 $s_t$ ，再给一个状态 $s_{t+1}$ ，要让网络输出的两个值 $V^\pi(s_t)$ 和 $V^\pi(s_{t+1})$ 的差越接近 $r_t$ 越好
  - 即![image-20210924183626791](https://gitee.com/Lighters_c/picgo-bed/raw/master/image-20210924183626791.png)

## Q-function

状态-动作价值函数，是另一种评论家，关注演员 $\pi$ 在状态 $s$ 时一定采取动作 $a$ ，后面就让 $\pi$ 自己玩下去得到的价值；

有两种写法

- 输入是状态跟动作，输出就是一个标量；
- 输入是一个状态，输出就是好几个值。



只要==有了这个 Q-function==，我们==就可以做强化学习==。有了这个 Q-function，我们就可以决定要采取哪一个动作，我们就可以进行`策略改进(Policy Improvement)`。

### 使用DQN强化学习

初始的演员叫做 $\pi$，这个 $\pi$跟环境互动，会收集数据。接下来你学习一个  $\pi$ 这个演员的 Q 值，你去衡量一下  $\pi$ 在某一个状态强制采取某一个动作，接下来用  $\pi$ 这个策略 会得到的期望奖励，用 TD 或 MC 都是可以的。你学习出一个 Q-function 以后，就保证你可以找到一个新的策略  $\pi'$ ，policy  $\pi'$ 一定会比原来的策略 $\pi$ 还要好。

### 如何定义策略好

我们说 $\pi'$ 一定会比 $\pi$  还要好，这边好是说，对所有可能的状态 s 而言，$V^{\pi'}(s)$ > $V^{\pi}(s)$。





## TD中会有两个网络

一个网络是 输入 $s_t$,$a_t$，输出 $Q^\pi(s_t, a_t)$ ，另一个网络是输入 $s_{t+1}$, $\pi(s_{t+1})$ ，如下：![image-20210924185823143](https://gitee.com/Lighters_c/picgo-bed/raw/master/image-20210924185823143.png)

会把其中一个 Q 网络，通常是你会把右边这个 Q 网络固定住。也就是说你在训练的时候，你只更新左边的 Q 网络的参数，而右边的 Q 网络的参数会被固定住。因为右边的 Q 网络负责产生目标，所以叫 `目标网络`。

因为目标网络是固定的，我们只调左边网络的参数，它就变成是一个回归问题。我们希望模型的输出的值跟目标越接近越好，你会最小化它的均方误差(mean square error)。

在实现的时候，你会把左边的 Q 网络更新好几次以后，再去用更新过的 Q 网络替换这个目标网络。